{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following experiment computes k-anonymity, l-diversity and t-closeness for trajectory data.\n",
    "\n",
    "The experiments is run on one-day worth of trip data from NYC yellow cabs.\n",
    "\n",
    "The metrics are at first computed on raw data and visualized for different sizes of equivalence areas.\n",
    "Later on, the data is anonymized and the metrics are computed on the anonymized data.\n",
    "Finally, metrics are compared across raw data and anonymized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from utils import subtract_polygons, extract_geom\n",
    "\n",
    "import json\n",
    "with open(\"../data/nyc_districts.json\") as json_file:\n",
    "    nyc_districts = json.load(json_file)\n",
    "nyc_districts = gpd.GeoDataFrame.from_features(nyc_districts['features'])\n",
    "nyc_districts['BoroCD'] = nyc_districts['BoroCD'].astype(str)\n",
    "\n",
    "\n",
    "# Define equivalence areas in NYC\n",
    "penn_station_poly = [(-73.99828815110244, 40.750805448135566), (-73.996228214579, 40.75356880601609), (-73.98779534943618,\n",
    "                                                                                                       40.7500251851189), (-73.98831033356704, 40.746725221537275), (-73.99828815110244, 40.750805448135566)]\n",
    "\n",
    "penn_station = gpd.GeoSeries(Polygon(penn_station_poly))\n",
    "penn_station = gpd.GeoDataFrame(\n",
    "    {'geometry': penn_station, 'BoroCD': 'penn_station'})\n",
    "\n",
    "grand_central_station_poly = [(-73.97714227694874, 40.7562458785223),\n",
    "                              (-73.98053258914356, 40.7513857387191),\n",
    "                              (-73.97669166583424, 40.749922750819835),\n",
    "                              (-73.97325843829518, 40.75475048873074),\n",
    "                              (-73.97714227694874, 40.7562458785223)]\n",
    "grand_central_station = gpd.GeoSeries(Polygon(grand_central_station_poly))\n",
    "grand_central_station = gpd.GeoDataFrame(\n",
    "    {'geometry': grand_central_station, 'BoroCD': 'grand_central_station'})\n",
    "\n",
    "port_authority_bus_poly = [(-73.99552889290536, 40.757026882235486), (-73.99063654366219, 40.754962609572274),\n",
    "                           (-73.98904867592537, 40.75720567448926), (-73.99402685585702, 40.75915610420332), (-73.99552889290536, 40.757026882235486)]\n",
    "\n",
    "port_authority_bus = gpd.GeoSeries(Polygon(port_authority_bus_poly))\n",
    "port_authority_bus = gpd.GeoDataFrame(\n",
    "    {'geometry': port_authority_bus, 'BoroCD': 'port_authority_bus'})\n",
    "\n",
    "la_guardia_airport_poly = [(-73.86330344855388, 40.765973691740584), (-73.87875297247966, 40.77302654459011),\n",
    "                           (-73.87390353858073, 40.77793384842783), (-73.85909774481853, 40.77003648640375), (-73.86330344855388, 40.765973691740584)]\n",
    "la_guardia_airport = gpd.GeoSeries(Polygon(la_guardia_airport_poly))\n",
    "la_guardia_airport = gpd.GeoDataFrame(\n",
    "    {'geometry': la_guardia_airport, 'BoroCD': 'la_guardia_airport'})\n",
    "\n",
    "jfk_airport_poly = [(-73.81083575955633, 40.64419826937766),(-73.7929829763532, 40.66340757736999),(-73.7644013570905, 40.64960348124331),(-73.78002254239324, 40.631627831232684),(-73.81083575955633, 40.64419826937766)]\n",
    "jfk_airport = gpd.GeoSeries(Polygon(jfk_airport_poly))\n",
    "jfk_airport = gpd.GeoDataFrame(\n",
    "    {'geometry': jfk_airport, 'BoroCD': 'jfk_airport'})\n",
    "\n",
    "battery_park_ferry_poly = [(-74.01721513018921, 40.71570211149159),(-74.01749407992676, 40.71375040736087),(-74.01363169894532, 40.71305103279016),(-74.01309525714234, 40.71516539855912),(-74.01721513018921, 40.71570211149159)]\n",
    "battery_park_ferry = gpd.GeoSeries(Polygon(battery_park_ferry_poly))\n",
    "battery_park_ferry = gpd.GeoDataFrame(\n",
    "    {'geometry': battery_park_ferry, 'BoroCD': 'battery_park_ferry'})\n",
    "\n",
    "whitehall_ferry_poly = [(-74.01369219413073, 40.700517009446806),(-74.01448612799913, 40.70320112120412),(-74.01158934226305, 40.7034613929551),(-74.01094561209948, 40.70090743242451),(-74.01369219413073, 40.700517009446806)]\n",
    "whitehall_ferry = gpd.GeoSeries(Polygon(whitehall_ferry_poly))\n",
    "whitehall_ferry = gpd.GeoDataFrame(\n",
    "    {'geometry': whitehall_ferry, 'BoroCD': 'whitehall_ferry'})\n",
    "\n",
    "wallstreet_ferry_poly = [(-74.00682730834231, 40.70238088314651),(-74.00894088904604, 40.704267853536116),(-74.00734229247317, 40.70532518426294),(-74.00508923690066, 40.7034138433873),(-74.00682730834231, 40.70238088314651)]\n",
    "wallstreet_ferry = gpd.GeoSeries(Polygon(wallstreet_ferry_poly))\n",
    "wallstreet_ferry = gpd.GeoDataFrame(\n",
    "    {'geometry': wallstreet_ferry, 'BoroCD': 'wallstreet_ferry'})\n",
    "\n",
    "midtown_ferry_poly = [(-74.00371185104896, 40.761439907892715),(-74.00519243042518, 40.75924574582725),(-74.00356164734413, 40.7586443702452),(-74.00186649124672, 40.76091981676726),(-74.00371185104896, 40.761439907892715)]\n",
    "midtown_ferry = gpd.GeoSeries(Polygon(midtown_ferry_poly))\n",
    "midtown_ferry = gpd.GeoDataFrame(\n",
    "    {'geometry': midtown_ferry, 'BoroCD': 'midtown_ferry'})\n",
    "\n",
    "# remove overlaps\n",
    "boro104 = subtract_polygons(nyc_districts, '104', [\n",
    "                            penn_station, port_authority_bus, midtown_ferry])\n",
    "boro105 = subtract_polygons(nyc_districts, '105', [\n",
    "                            penn_station, grand_central_station, port_authority_bus])\n",
    "boro106 = subtract_polygons(nyc_districts, '106', [grand_central_station])\n",
    "boro480 = subtract_polygons(nyc_districts, '480', [la_guardia_airport])\n",
    "boro403 = subtract_polygons(nyc_districts, '403', [la_guardia_airport])\n",
    "boro483 = subtract_polygons(nyc_districts, '483', [jfk_airport])\n",
    "boro101 = subtract_polygons(nyc_districts, '101', [battery_park_ferry,whitehall_ferry,wallstreet_ferry])\n",
    "\n",
    "nyc_districts = pd.concat([penn_station,\n",
    "                           grand_central_station,\n",
    "                           port_authority_bus,\n",
    "                           la_guardia_airport,\n",
    "                           jfk_airport,\n",
    "                           battery_park_ferry,\n",
    "                           whitehall_ferry,\n",
    "                           wallstreet_ferry,\n",
    "                           midtown_ferry,\n",
    "                           boro101,\n",
    "                           boro104,\n",
    "                           boro105,\n",
    "                           boro106,\n",
    "                           boro403,\n",
    "                           boro480,\n",
    "                           boro483,\n",
    "                           nyc_districts[~nyc_districts['BoroCD'].isin(\n",
    "                               ['101', '104', '105', '106', '403', '480', '483'])]\n",
    "                           ])\n",
    "eq_areas = extract_geom(nyc_districts)\n",
    "\n",
    "len(eq_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all districts\n",
    "from utils import plot_choropleth_polygon\n",
    "plot_choropleth_polygon(pd.DataFrame({'cluster_id':nyc_districts.BoroCD.unique(),'cnt':range(nyc_districts.BoroCD.nunique())}),\n",
    "nyc_districts,field='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = pd.read_csv(\"../data/nyc_taxi_raw.csv.gz\", compression=\"gzip\",\n",
    "                   parse_dates=['start_time', 'end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only one day worth of data\n",
    "from datetime import datetime\n",
    "taxi = taxi[(taxi['start_time'] >= datetime(year=2009, month=1, day=5)) &\n",
    "            (taxi['start_time'] < datetime(year=2009, month=1, day=6))]\n",
    "taxi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "for col in taxi.columns:\n",
    "    taxi[col].replace('', np.nan, inplace=True)\n",
    "    taxi.dropna(subset=[col], inplace=True)\n",
    "taxi['start_longitude'] = taxi['start_longitude'].astype(float)\n",
    "taxi['start_latitude'] = taxi['start_latitude'].astype(float)\n",
    "taxi['end_longitude'] = taxi['end_longitude'].astype(float)\n",
    "taxi['end_latitude'] = taxi['end_latitude'].astype(float)\n",
    "taxi = taxi[taxi['start_longitude'] != 0]\n",
    "taxi = taxi[taxi['start_latitude'] != 0]\n",
    "taxi = taxi[taxi['end_longitude'] != 0]\n",
    "taxi = taxi[taxi['end_latitude'] != 0]\n",
    "taxi = taxi[(taxi['start_longitude'] >= -180) &\n",
    "            (taxi['start_longitude'] <= 180)]\n",
    "taxi = taxi[(taxi['end_longitude'] >= -180) & (taxi['end_longitude'] <= 180)]\n",
    "taxi = taxi[(taxi['start_latitude'] >= -90) & (taxi['start_latitude'] <= 90)]\n",
    "taxi = taxi[(taxi['end_latitude'] >= -90) & (taxi['end_latitude'] <= 90)]\n",
    "# add random ID\n",
    "taxi[\"trajectory_id\"] = [uuid.uuid4() for _ in range(len(taxi.index))]\n",
    "# sort\n",
    "taxi = taxi.sort_values(by=['trajectory_id', 'start_time'])\n",
    "taxi.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit data to within a (conservatively large) NYC bbox\n",
    "lonW = -74.67\n",
    "lonE = -71.75\n",
    "latN = 41.50\n",
    "latS = 40.30\n",
    "taxi = taxi[(taxi['start_longitude'] >= lonW) &\n",
    "            (taxi['start_longitude'] <= lonE)]\n",
    "taxi = taxi[(taxi['end_longitude'] >= lonW) & (taxi['end_longitude'] <= lonE)]\n",
    "taxi = taxi[(taxi['start_latitude'] >= latS) &\n",
    "            (taxi['start_latitude'] <= latN)]\n",
    "taxi = taxi[(taxi['end_latitude'] >= latS) & (taxi['end_latitude'] <= latN)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove short trips\n",
    "from datetime import timedelta\n",
    "taxi['duration'] = (taxi.end_time-taxi.start_time)\n",
    "taxi = taxi[taxi['duration'] > timedelta(seconds=60)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unix timestamps\n",
    "taxi['start_time_unix'] = (taxi.start_time - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "taxi['end_time_unix'] = (taxi.end_time - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save, including random trip IDs\n",
    "taxi.to_csv(\"../data/trips.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxi = pd.read_csv(\"../data/trips.csv.gz\", compression='gzip', parse_dates=[\"start_time\", \"end_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics with grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "params = {\"spatial_threshold\": [0.002, 0.005, 0.01],\n",
    "          \"temporal_threshold\": [300, 600, 1800]}\n",
    "experiment_conditions = [{k: v for k, v in zip(\n",
    "    params.keys(), x)} for x in product(*params.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import body\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "nthreads = None\n",
    "\n",
    "if (nthreads is not None) and (nthreads > 0) and (__name__ == \"__main__\"):\n",
    "    pool = Pool(processes=min(cpu_count()-1,\n",
    "                              min(nthreads, len(experiment_conditions))))\n",
    "    results = pool.map(partial(body, df=taxi), experiment_conditions)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "else:\n",
    "    results = list(map(partial(body, df=taxi), experiment_conditions))\n",
    "results = pd.concat(results)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"../data/results.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results=pd.read_csv(\"../data/results.csv.gz\",compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count number of trips in each time window\n",
    "\n",
    "To determine which time windows correspond to rush-hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results of the following experiment\n",
    "spatial_param = 0.005  # 0^{\\circ} 0' 18''\n",
    "temporal_param = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid in space and time to reference the data\n",
    "from utils import define_equivalence_areas\n",
    "_, bboxes = define_equivalence_areas(\n",
    "    taxi, spatial_param, temporal_param, return_bbox=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find how many trajectories fall into each 1-h time window (across all the cells)\n",
    "ret = {}\n",
    "cells = results[(results.spatial_threshold == spatial_param) &\n",
    "               (results.temporal_threshold == temporal_param)].set_index('cluster_id')\n",
    "for h in range(0, 23):\n",
    "    twin_limits = (int(pd.datetime(year=2009, month=1, day=5, hour=h).timestamp()),\n",
    "           int(pd.datetime(year=2009, month=1, day=5, hour=h+1).timestamp()))\n",
    "    cells_in_twin = bboxes[bboxes['time_box'].apply(lambda t: len(\n",
    "        set(range(*t)).intersection(set(range(*twin_limits)))) > 0)]\n",
    "    ret.update({f\"{h}-{h+1}\": pd.merge(cells_in_twin, cells, left_index=True,\n",
    "                                       right_index=True, how='inner').shape[0]})\n",
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot staircase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils import plot_staircase\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ymax = max(results.k_anonymity.max(), results.l_diversity.max())\n",
    "for i, st in enumerate(params['spatial_threshold']):\n",
    "    for j, tt in enumerate(params['temporal_threshold']):\n",
    "        ax = fig.add_subplot(len(set(params['spatial_threshold'])),\n",
    "                             len(set(params['temporal_threshold'])),\n",
    "                             len(set(params['temporal_threshold']))*i+j+1)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f\"Spatial threshold: {st}\", size=18)\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"Temporal threshold: {int(tt/60)} mins\", size=18)\n",
    "        if i == len(set(params['spatial_threshold']))-1:\n",
    "            ax.set_xlabel('Percentage of traces', fontsize=14)\n",
    "        ax.set_ylim(0, ymax)\n",
    "        ax.yaxis.tick_right()\n",
    "        plot_staircase(results.query(\n",
    "            f\"spatial_threshold == {st} & temporal_threshold == {tt}\"), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils import plot_tclose\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "tmax = results.t_closeness.max()\n",
    "for i, st in enumerate(params['spatial_threshold']):\n",
    "    for j, tt in enumerate(params['temporal_threshold']):\n",
    "        ax = fig.add_subplot(len(set(params['spatial_threshold'])),\n",
    "                             len(set(params['temporal_threshold'])),\n",
    "                             len(set(params['temporal_threshold']))*i+j+1)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f\"Spatial threshold: {st}\", size=18)\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"Temporal threshold: {int(tt/60)} mins\", size=18)\n",
    "        if i == len(set(params['spatial_threshold']))-1:\n",
    "            ax.set_xlabel('Percentage of traces', fontsize=14)\n",
    "#         ax.set_ylim(0,tmax)\n",
    "        ax.set_yscale('log')\n",
    "        ax.yaxis.tick_right()\n",
    "        plot_tclose(results.query(\n",
    "            f\"spatial_threshold == {st} & temporal_threshold == {tt}\"), ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "params_anon = {\"spatial_threshold\": [0.005],\n",
    "               \"temporal_threshold\": [600],\n",
    "               # anonymization parameters\n",
    "               \"spatial_noise\": [0.001, 0.005, 0.02],\n",
    "               \"temporal_noise\": [100, 600, 2400],\n",
    "               # repetitions\n",
    "               \"nrep\": list(range(3))}\n",
    "anon_conditions = [{k: v for k, v in zip(\n",
    "    params_anon.keys(), x)} for x in product(*params_anon.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from utils import anon_body\n",
    "nthreads = None\n",
    "\n",
    "if (nthreads is not None) and (nthreads > 0) and (__name__ == \"__main__\"):\n",
    "    pool = Pool(processes=min(cpu_count()-1,\n",
    "                              min(nthreads, len(anon_conditions))))\n",
    "    anon_results = pool.map(partial(anon_body, df=taxi), anon_conditions)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "else:\n",
    "    anon_results = list(map(partial(anon_body, df=taxi), anon_conditions))\n",
    "anon_results = pd.concat(anon_results)\n",
    "len(anon_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_results.to_csv(\"../data/anon_results.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anon_results=pd.read_csv(\"../data/anon_results.csv.gz\",compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results of the following experiment\n",
    "spatial_param = 0.005\n",
    "temporal_param = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_noise = spatial_param\n",
    "temporal_noise = temporal_param\n",
    "target_data = anon_results[(anon_results['spatial_threshold'] == spatial_param) &\n",
    "                           (anon_results['temporal_threshold'] == temporal_param) &\n",
    "                           (anon_results['spatial_noise'] == spatial_noise) &\n",
    "                           (anon_results['temporal_noise'] == temporal_noise)]\n",
    "reference_data = results[(results['spatial_threshold'] == spatial_param) &\n",
    "                         (results['temporal_threshold'] == temporal_param)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_anon = scores_time_window(target_data, taxi, spatial_param, temporal_param,\n",
    "                              (int(taxi.start_time.min().timestamp()),\n",
    "                               int(taxi.end_time.max().timestamp())))\n",
    "vis_anon['l_div_norm'] = vis_anon['l_diversity']/vis_anon['k_anonymity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_choropleth\n",
    "plot_choropleth(vis_anon, field='k_anonymity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_anon_morning=scores_time_window(target_data,taxi,spatial_param,temporal_param,time_window_morning)\n",
    "# vis_anon_morning['l_div_norm']=vis_anon_morning['l_diversity']/vis_anon_morning['k_anonymity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_anon_evening=scores_time_window(target_data,taxi,spatial_param,temporal_param,time_window_evening)\n",
    "# vis_anon_evening['l_div_norm']=vis_anon_evening['l_diversity']/vis_anon_evening['k_anonymity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_choropleth(vis_anon_morning,field='k_anonymity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_choropleth(vis_anon_morning,field='l_div_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_choropleth(vis_anon_evening,field='k_anonymity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_choropleth(vis_anon_evening,field='l_div_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the difference in scores between raw and anon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pd.merge(reference_data[['cluster_id', 'l_diversity', 'k_anonymity', 't_closeness', 'spatial_threshold', 'temporal_threshold']\n",
    "                               ].rename(columns={'k_anonymity': 'k_orig', 'l_diversity': 'l_orig', 't_closeness': 't_orig'}).drop_duplicates(),\n",
    "                target_data[['cluster_id', 'nrep', 'l_diversity', 'k_anonymity', 't_closeness']\n",
    "                            ].drop_duplicates().rename(columns={'k_anonymity': 'k_anon', 'l_diversity': 'l_anon', 't_closeness': 't_anon'}),\n",
    "                on=['cluster_id'], how='outer')\n",
    "diff['k_anonymity'] = diff['k_anon']-diff['k_orig']\n",
    "diff['l_diversity'] = diff['l_anon']-diff['l_orig']\n",
    "diff['t_closeness'] = diff['t_anon']-diff['t_orig']\n",
    "diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.groupby('cluster_id').agg(np.mean)[['k_anon', 'k_orig']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_diff = scores_time_window(diff, taxi, spatial_param, temporal_param,\n",
    "                              (int(taxi.start_time.min().timestamp()),\n",
    "                               int(taxi.end_time.max().timestamp())))\n",
    "vis_diff['l_div_norm'] = vis_diff['l_diversity']/vis_diff['k_anonymity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_choropleth\n",
    "plot_choropleth(vis_diff.dropna(), field='k_anonymity', cmap='PuOr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics with polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_param=3600 #1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# precompute clusters to save time\n",
    "from utils import define_equivalence_areas_polygons\n",
    "taxi=define_equivalence_areas_polygons(taxi, eq_areas, temporal_param)\n",
    "taxi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all traces that start and end in the same equivalence area\n",
    "taxi=taxi[taxi.equivalence_area_start!=taxi.equivalence_area_end]\n",
    "taxi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.to_csv(\"../data/trips_clustered.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import body_compute_metrics\n",
    "results_polygons=body_compute_metrics(clustered_data=taxi)\n",
    "results_polygons['temporal_threshold']=temporal_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_polygons.to_csv(\"../data/results_polygons.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_polygons=pd.read_csv(\"../data/results_polygons.csv.gz\",compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot heatmap for the whole day, hourly scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import scores_time_window_polygon\n",
    "vis = scores_time_window_polygon(results_polygons, taxi, eq_areas, temporal_param,\n",
    "                         (int(taxi.start_time.min().timestamp()),\n",
    "                          int(taxi.end_time.max().timestamp())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import plot_choropleth_polygon\n",
    "plot_choropleth_polygon(vis, nyc_districts, field='k_anonymity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot heatmap for the rush hour times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window_morning = (int(pd.datetime(year=2009, month=1, day=5, hour=7).timestamp()),\n",
    "                       int(pd.datetime(year=2009, month=1, day=5, hour=7, minute=30).timestamp()))\n",
    "time_window_evening = (int(pd.datetime(year=2009, month=1, day=5, hour=17).timestamp()),\n",
    "                       int(pd.datetime(year=2009, month=1, day=5, hour=18).timestamp()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from utils import get_containing_polygon_name\n",
    "def get_results_timewindow(df,time_window,eq_areas):\n",
    "    tqdm.pandas(desc=\"Find data that fits in the time window\")\n",
    "    ret = df[df.progress_apply(lambda t: len(set(range(t.start_time_unix,t.end_time_unix)\n",
    "                                                ).intersection(set(range(*time_window)))) > 0,axis=1)].copy()\n",
    "    # build area names\n",
    "    tqdm.pandas(desc=\"Match starting points with polygons\")\n",
    "    ret.loc[:,'equivalence_area_start'] = ret.progress_apply(lambda r: get_containing_polygon_name(\n",
    "        r.start_latitude, r.start_longitude, eq_areas), axis=1)\n",
    "    tqdm.pandas(desc=\"Match ending points with polygons\")\n",
    "    ret.loc[:,'equivalence_area_end'] = ret.progress_apply(lambda r: get_containing_polygon_name(\n",
    "        r.end_latitude, r.end_longitude, eq_areas), axis=1)\n",
    "    ret = ret[~((ret.equivalence_area_start.isin([\"NA\"])) | (ret.equivalence_area_end.isin([\"NA\"])))]\n",
    "    # remove all traces that start and end in the same equivalence area\n",
    "    ret=ret[ret.equivalence_area_start!=ret.equivalence_area_end]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_morning = get_results_timewindow(taxi,time_window_morning,eq_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import body_compute_metrics\n",
    "results_morning=body_compute_metrics(clustered_data=data_morning)\n",
    "results_morning.to_csv(\"../data/results_morning.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_morning=results_morning.groupby('cluster_id').agg({'k_anonymity': np.mean, 'l_diversity': np.mean, 't_closeness': np.mean, 'l_div_norm': np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_morning=results_morning[results_morning.k_anonymity>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_choropleth_polygon\n",
    "plot_choropleth_polygon(results_morning, nyc_districts, field='k_anonymity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import plot_choropleth_polygon\n",
    "plot_choropleth_polygon(results_morning, nyc_districts, field='l_div_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_evening = get_results_timewindow(taxi,time_window_evening,eq_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import body_compute_metrics\n",
    "results_evening=body_compute_metrics(clustered_data=data_evening)\n",
    "results_evening.to_csv(\"../data/results_evening.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_evening=results_evening.groupby('cluster_id').agg({'k_anonymity': np.mean, 'l_diversity': np.mean, 't_closeness': np.mean, 'l_div_norm': np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_evening=results_evening[results_evening.k_anonymity>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_choropleth_polygon(results_evening, nyc_districts, field='k_anonymity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_choropleth_polygon(results_evening, nyc_districts, field='l_div_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_noise_to_data\n",
    "def anonymization_fct(e,df):\n",
    "    df['start_time']=pd.to_datetime(df.start_time)\n",
    "    df['end_time']=pd.to_datetime(df.end_time)\n",
    "    ret=add_noise_to_data(e,df)\n",
    "    for k, v in e.items():\n",
    "        ret[k] = v\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "params_anon = {# anonymization parameters\n",
    "               \"spatial_noise\": [0.005], #[0.001, 0.005, 0.02],\n",
    "               \"temporal_noise\": [600], #[100, 600, 2400],\n",
    "               # repetitions\n",
    "               \"nrep\": list(range(3))}\n",
    "anon_conditions = [{k: v for k, v in zip(\n",
    "    params_anon.keys(), x)} for x in product(*params_anon.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "nthreads = None\n",
    "\n",
    "if (nthreads is not None) and (nthreads > 0) and (__name__ == \"__main__\"):\n",
    "    pool = Pool(processes=min(cpu_count()-1,\n",
    "                              min(nthreads, len(anon_conditions))))\n",
    "    anon_data = pool.map(partial(anonymization_fct, df=taxi), anon_conditions)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "else:\n",
    "    anon_data = list(map(partial(anonymization_fct, df=taxi), anon_conditions))\n",
    "anon_data = pd.concat(anon_data)\n",
    "len(anon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_data.to_csv(\"../data/anon_data.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anon_data=pd.read_csv(\"../data/anon_data.csv.gz\",compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results of the following experiment\n",
    "spatial_noise = 0.005\n",
    "temporal_noise = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = anon_data[(anon_data['spatial_noise'] == spatial_noise) &\n",
    "                           (anon_data['temporal_noise'] == temporal_noise)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_data_morning = get_results_timewindow(target_data,time_window_morning,eq_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import body_compute_metrics\n",
    "anon_results_morning=[]\n",
    "for r in anon_data_morning.nrep.unique():\n",
    "    tmp=anon_data_morning[anon_data_morning.nrep==r]\n",
    "    ret=body_compute_metrics(clustered_data=tmp)\n",
    "    ret['nrep']=r\n",
    "    anon_results_morning+=[ret]\n",
    "anon_results_morning=pd.concat(anon_results_morning)\n",
    "anon_results_morning.to_csv(\"../data/anon_results_morning.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_results_morning=anon_results_morning[anon_results_morning.k_anonymity>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_results_morning=anon_results_morning.groupby('cluster_id').agg({'k_anonymity': np.mean, 'l_diversity': np.mean, 't_closeness': np.mean, 'l_div_norm': np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_choropleth_polygon\n",
    "plot_choropleth_polygon(anon_results_morning, nyc_districts, field='k_anonymity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import plot_choropleth_polygon\n",
    "plot_choropleth_polygon(anon_results_morning, nyc_districts, field='l_div_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_data_evening = get_results_timewindow(target_data,time_window_evening,eq_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import body_compute_metrics\n",
    "anon_results_evening=[]\n",
    "for r in anon_data_evening.nrep.unique():\n",
    "    tmp=anon_data_evening[anon_data_evening.nrep==r]\n",
    "    ret=body_compute_metrics(clustered_data=tmp)\n",
    "    ret['nrep']=r\n",
    "    anon_results_evening+=[ret]\n",
    "anon_results_evening=pd.concat(anon_results_evening)\n",
    "anon_results_evening.to_csv(\"../data/anon_results_evening.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_results_evening=anon_results_evening[anon_results_evening.k_anonymity>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_results_evening=anon_results_evening.groupby('cluster_id').agg({'k_anonymity': np.mean, 'l_diversity': np.mean, 't_closeness': np.mean, 'l_div_norm': np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_choropleth_polygon(anon_results_evening, nyc_districts, field='k_anonymity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_choropleth_polygon(anon_results_evening, nyc_districts, field='l_div_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the difference in scores between raw and anon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_morning = pd.merge(results_morning[['l_diversity', 'k_anonymity', 't_closeness', 'l_div_norm']\n",
    "                               ].rename(columns={'k_anonymity': 'k_orig', 'l_diversity': 'l_orig', 't_closeness': 't_orig', 'l_div_norm': 'l_norm_orig'}).drop_duplicates(),\n",
    "                anon_results_morning[['l_diversity', 'k_anonymity', 't_closeness', 'l_div_norm']\n",
    "                            ].drop_duplicates().rename(columns={'k_anonymity': 'k_anon', 'l_diversity': 'l_anon', 't_closeness': 't_anon', 'l_div_norm': 'l_norm_anon'}),\n",
    "                right_index=True, left_index=True, how='outer')\n",
    "diff_morning['k_anonymity'] = diff_morning['k_anon']-diff_morning['k_orig']\n",
    "diff_morning['l_diversity'] = diff_morning['l_anon']-diff_morning['l_orig']\n",
    "diff_morning['t_closeness'] = diff_morning['t_anon']-diff_morning['t_orig']\n",
    "diff_morning['l_div_norm'] = diff_morning['l_norm_anon']-diff_morning['l_norm_orig']\n",
    "diff_morning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_choropleth_polygon(diff_morning.dropna(), nyc_districts, field='k_anonymity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_choropleth_polygon(diff_morning.dropna(), nyc_districts, field='l_div_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_evening = pd.merge(results_evening[['l_diversity', 'k_anonymity', 't_closeness', 'l_div_norm']\n",
    "                               ].rename(columns={'k_anonymity': 'k_orig', 'l_diversity': 'l_orig', 't_closeness': 't_orig', 'l_div_norm': 'l_norm_orig'}).drop_duplicates(),\n",
    "                anon_results_evening[['l_diversity', 'k_anonymity', 't_closeness', 'l_div_norm']\n",
    "                            ].drop_duplicates().rename(columns={'k_anonymity': 'k_anon', 'l_diversity': 'l_anon', 't_closeness': 't_anon', 'l_div_norm': 'l_norm_anon'}),\n",
    "                right_index=True, left_index=True, how='outer')\n",
    "diff_evening['k_anonymity'] = diff_evening['k_anon']-diff_evening['k_orig']\n",
    "diff_evening['l_diversity'] = diff_evening['l_anon']-diff_evening['l_orig']\n",
    "diff_evening['t_closeness'] = diff_evening['t_anon']-diff_evening['t_orig']\n",
    "diff_evening['l_div_norm'] = diff_evening['l_norm_anon']-diff_evening['l_norm_orig']\n",
    "diff_evening.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_choropleth_polygon(diff_evening.dropna(), nyc_districts, field='k_anonymity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_choropleth_polygon(diff_evening.dropna(), nyc_districts, field='l_div_norm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
